{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rohi.rockz180@gmail.com_SO_tagpredictor_Models",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5Plrr2NU93P",
        "colab_type": "text"
      },
      "source": [
        "<img src='images/pic1.jpg'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAZDkLNPU93Y",
        "colab_type": "text"
      },
      "source": [
        "# Stack Overflow: Tag Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c73f6d7c-e0ec-43a9-89b6-373b0ef5ccaf",
        "id": "Nh_S7GRm3sZv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip3 install scikit-multilearn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-multilearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/1f/e6ff649c72a1cdf2c7a1d31eb21705110ce1c5d3e7e26b2cc300e1637272/scikit_multilearn-0.2.0-py3-none-any.whl (89kB)\n",
            "\r\u001b[K     |███▊                            | 10kB 25.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 20kB 4.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 40kB 3.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 51kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 61kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 71kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 81kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 5.0MB/s \n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihs1Hb3RU93S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import os\n",
        "from sqlalchemy import create_engine # database connection\n",
        "import datetime as dt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score,precision_score,recall_score\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from skmultilearn.adapt import mlknn\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8bHKOMJtQhJ",
        "colab_type": "code",
        "outputId": "e36009ec-6be3-4b1a-d1c9-6e32c759f458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-Z7F0_mU97F",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<h3>  Cleaning and preprocessing of Questions </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWzF-nN6U97G",
        "colab_type": "text"
      },
      "source": [
        "<h3> Preprocessing </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeR1aoQaU97H",
        "colab_type": "text"
      },
      "source": [
        "<ol> \n",
        "    <li> Sample 0.5M data points </li>\n",
        "    <li> Separate out code-snippets from Body </li>\n",
        "    <li> Remove Spcial characters from Question title and description (not in code)</li>\n",
        "    <li> Remove stop words (Except 'C') </li>\n",
        "    <li> Remove HTML Tags </li>\n",
        "    <li> Convert all the characters into small letters </li>\n",
        "    <li> Use SnowballStemmer to stem the words </li>\n",
        "    <li> <b> Give more weightage to title : Add title three times to the question </b> </li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2A__qlu8--g",
        "colab_type": "code",
        "outputId": "cc07306f-1978-4579-d9d1-bbfddc8c5d50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr2xhpsAU97I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def striphtml(data):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', str(data))\n",
        "    return cleantext\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCDUa4KxU97L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#http://www.sqlitetutorial.net/sqlite-python/create-tables/\n",
        "def create_connection(db_file):\n",
        "    \"\"\" create a database connection to the SQLite database\n",
        "        specified by db_file\n",
        "    :param db_file: database file\n",
        "    :return: Connection object or None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_file)\n",
        "        return conn\n",
        "    except Error as e:\n",
        "        print(e)\n",
        " \n",
        "    return None\n",
        "\n",
        "def create_table(conn, create_table_sql):\n",
        "    \"\"\" create a table from the create_table_sql statement\n",
        "    :param conn: Connection object\n",
        "    :param create_table_sql: a CREATE TABLE statement\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    try:\n",
        "        c = conn.cursor()\n",
        "        c.execute(create_table_sql)\n",
        "    except Error as e:\n",
        "        print(e)\n",
        "        \n",
        "def checkTableExists(dbcon):\n",
        "    cursr = dbcon.cursor()\n",
        "    str = \"select name from sqlite_master where type='table'\"\n",
        "    table_names = cursr.execute(str)\n",
        "    print(\"Tables in the databse:\")\n",
        "    tables =table_names.fetchall() \n",
        "    print(tables[0][0])\n",
        "    return(len(tables))\n",
        "\n",
        "def create_database_table(database, query):\n",
        "    conn = create_connection(database)\n",
        "    if conn is not None:\n",
        "        create_table(conn, query)\n",
        "        checkTableExists(conn)\n",
        "    else:\n",
        "        print(\"Error! cannot create the database connection.\")\n",
        "    conn.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB0bL2drU97w",
        "colab_type": "text"
      },
      "source": [
        "<h1> Machine Learning Models </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvjtTBZ6U98y",
        "colab_type": "text"
      },
      "source": [
        "<h2>  Modeling with less data points (0.5M data points) and more weight to title and 500 tags only. </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0QKMrEwU98y",
        "colab_type": "code",
        "outputId": "3f1295d3-1aac-4cf0-f7a7-fab13cab6d6d",
        "colab": {}
      },
      "source": [
        "sql_create_table = \"\"\"CREATE TABLE IF NOT EXISTS QuestionsProcessed (question text NOT NULL, code text, tags text, words_pre integer, words_post integer, is_code integer);\"\"\"\n",
        "create_database_table(\"Titlemoreweight.db\", sql_create_table)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tables in the databse:\n",
            "QuestionsProcessed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHLunpYsU982",
        "colab_type": "code",
        "outputId": "e79456dd-e137-4e91-8171-8379f4c0c286",
        "colab": {}
      },
      "source": [
        "# http://www.sqlitetutorial.net/sqlite-delete/\n",
        "# https://stackoverflow.com/questions/2279706/select-random-row-from-a-sqlite-table\n",
        "\n",
        "read_db = 'train_no_dup.db'\n",
        "write_db = 'Titlemoreweight.db'\n",
        "train_datasize = 400000\n",
        "if os.path.isfile(read_db):\n",
        "    conn_r = create_connection(read_db)\n",
        "    if conn_r is not None:\n",
        "        reader =conn_r.cursor()\n",
        "        # for selecting first 0.5M rows\n",
        "        reader.execute(\"SELECT Title, Body, Tags From no_dup_train LIMIT 500001;\")\n",
        "        # for selecting random points\n",
        "        #reader.execute(\"SELECT Title, Body, Tags From no_dup_train ORDER BY RANDOM() LIMIT 500001;\")\n",
        "\n",
        "if os.path.isfile(write_db):\n",
        "    conn_w = create_connection(write_db)\n",
        "    if conn_w is not None:\n",
        "        tables = checkTableExists(conn_w)\n",
        "        writer =conn_w.cursor()\n",
        "        if tables != 0:\n",
        "            writer.execute(\"DELETE FROM QuestionsProcessed WHERE 1\")\n",
        "            print(\"Cleared All the rows\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tables in the databse:\n",
            "QuestionsProcessed\n",
            "Cleared All the rows\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifSmL0M-U98-",
        "colab_type": "code",
        "outputId": "78019f2f-3a06-4b6d-a761-c8cae342ac42",
        "colab": {}
      },
      "source": [
        "#http://www.bernzilla.com/2008/05/13/selecting-a-random-row-from-an-sqlite-table/\n",
        "start = datetime.now()\n",
        "preprocessed_data_list=[]\n",
        "reader.fetchone()\n",
        "questions_with_code=0\n",
        "len_pre=0\n",
        "len_post=0\n",
        "questions_proccesed = 0\n",
        "for row in reader:\n",
        "    \n",
        "    is_code = 0\n",
        "    \n",
        "    title, question, tags = row[0], row[1], str(row[2])\n",
        "    \n",
        "    if '<code>' in question:\n",
        "        questions_with_code+=1\n",
        "        is_code = 1\n",
        "    x = len(question)+len(title)\n",
        "    len_pre+=x\n",
        "    \n",
        "    code = str(re.findall(r'<code>(.*?)</code>', question, flags=re.DOTALL))\n",
        "    \n",
        "    question=re.sub('<code>(.*?)</code>', '', question, flags=re.MULTILINE|re.DOTALL)\n",
        "    question=striphtml(question.encode('utf-8'))\n",
        "    \n",
        "    title=title.encode('utf-8')\n",
        "    \n",
        "    # adding title three time to the data to increase its weight\n",
        "    # add tags string to the training data\n",
        "    \n",
        "    question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+question\n",
        "    \n",
        "#     if questions_proccesed<=train_datasize:\n",
        "#         question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+question+\" \"+str(tags)\n",
        "#     else:\n",
        "#         question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+question\n",
        "\n",
        "    question=re.sub(r'[^A-Za-z0-9#+.\\-]+',' ',question)\n",
        "    words=word_tokenize(str(question.lower()))\n",
        "    \n",
        "    #Removing all single letter and and stopwords from question exceptt for the letter 'c'\n",
        "    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n",
        "    \n",
        "    len_post+=len(question)\n",
        "    tup = (question,code,tags,x,len(question),is_code)\n",
        "    questions_proccesed += 1\n",
        "    writer.execute(\"insert into QuestionsProcessed(question,code,tags,words_pre,words_post,is_code) values (?,?,?,?,?,?)\",tup)\n",
        "    if (questions_proccesed%100000==0):\n",
        "        print(\"number of questions completed=\",questions_proccesed)\n",
        "\n",
        "no_dup_avg_len_pre=(len_pre*1.0)/questions_proccesed\n",
        "no_dup_avg_len_post=(len_post*1.0)/questions_proccesed\n",
        "\n",
        "print( \"Avg. length of questions(Title+Body) before processing: %d\"%no_dup_avg_len_pre)\n",
        "print( \"Avg. length of questions(Title+Body) after processing: %d\"%no_dup_avg_len_post)\n",
        "print (\"Percent of questions containing code: %d\"%((questions_with_code*100.0)/questions_proccesed))\n",
        "\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of questions completed= 100000\n",
            "number of questions completed= 200000\n",
            "number of questions completed= 300000\n",
            "number of questions completed= 400000\n",
            "number of questions completed= 500000\n",
            "Avg. length of questions(Title+Body) before processing: 1239\n",
            "Avg. length of questions(Title+Body) after processing: 424\n",
            "Percent of questions containing code: 57\n",
            "Time taken to run this cell : 0:23:12.329039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x54WQvZAU99B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# never forget to close the conections or else we will end up with database locks\n",
        "conn_r.commit()\n",
        "conn_w.commit()\n",
        "conn_r.close()\n",
        "conn_w.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IspyyegoU99N",
        "colab_type": "text"
      },
      "source": [
        "__ Saving Preprocessed data to a Database __"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpPE55jFcDuF",
        "colab_type": "text"
      },
      "source": [
        "## Taking only 100k data points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_x-ETQJU99P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#0.5 Million entries are in the db out of which we take only 100k entries to dataframe.\n",
        "write_db = 'gdrive/My Drive/data/Titlemoreweight.db'\n",
        "if os.path.isfile(write_db):\n",
        "    conn_r = create_connection(write_db)\n",
        "    if conn_r is not None:\n",
        "        preprocessed_data = pd.read_sql_query(\"\"\"SELECT question, Tags FROM QuestionsProcessed LIMIT 100000\"\"\", conn_r)\n",
        "conn_r.commit()\n",
        "conn_r.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc7hwHjBU99U",
        "colab_type": "code",
        "outputId": "b8ce671f-c3c8-416f-db2d-23e51ee863ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "preprocessed_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dynam datagrid bind silverlight dynam datagrid...</td>\n",
              "      <td>c# silverlight data-binding</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dynam datagrid bind silverlight dynam datagrid...</td>\n",
              "      <td>c# silverlight data-binding columns</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>java.lang.noclassdeffounderror javax servlet j...</td>\n",
              "      <td>jsp jstl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>java.sql.sqlexcept microsoft odbc driver manag...</td>\n",
              "      <td>java jdbc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>better way updat feed fb php sdk better way up...</td>\n",
              "      <td>facebook api facebook-php-sdk</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question                                 tags\n",
              "0  dynam datagrid bind silverlight dynam datagrid...          c# silverlight data-binding\n",
              "1  dynam datagrid bind silverlight dynam datagrid...  c# silverlight data-binding columns\n",
              "2  java.lang.noclassdeffounderror javax servlet j...                             jsp jstl\n",
              "3  java.sql.sqlexcept microsoft odbc driver manag...                            java jdbc\n",
              "4  better way updat feed fb php sdk better way up...        facebook api facebook-php-sdk"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk9V0azqU99X",
        "colab_type": "code",
        "outputId": "ef68d2c8-37de-4669-cdf7-5e33e3ad900a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"number of data points in sample :\", preprocessed_data.shape[0])\n",
        "print(\"number of dimensions :\", preprocessed_data.shape[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of data points in sample : 100000\n",
            "number of dimensions : 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ3-VPbqU97w",
        "colab_type": "text"
      },
      "source": [
        "<h2>  Converting tags for multilabel problems </h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K88oaTD9U97y",
        "colab_type": "text"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "<th>X</th><th>y1</th><th>y2</th><th>y3</th><th>y4</th>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>x1</td><td>0</td><td>1</td><td>1</td><td>0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>x1</td><td>1</td><td>0</td><td>0</td><td>0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>x1</td><td>0</td><td>1</td><td>0</td><td>0</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VCwyfHxU972",
        "colab_type": "text"
      },
      "source": [
        "__ We will sample the number of tags instead considering all of them (due to limitation of computing power) __"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FtgktWvU973",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tags_to_choose(n):\n",
        "    t = multilabel_y.sum(axis=0).tolist()[0]\n",
        "    sorted_tags_i = sorted(range(len(t)), key=lambda i: t[i], reverse=True)\n",
        "    multilabel_yn=multilabel_y[:,sorted_tags_i[:n]]\n",
        "    return multilabel_yn\n",
        "\n",
        "def questions_explained_fn(n):\n",
        "    multilabel_yn = tags_to_choose(n)\n",
        "    x= multilabel_yn.sum(axis=1)\n",
        "    return (np.count_nonzero(x==0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUpccCSkU99Z",
        "colab_type": "text"
      },
      "source": [
        "__ Converting string Tags to multilable output variables __ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWg_g1lNU99a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n",
        "multilabel_y = vectorizer.fit_transform(preprocessed_data['tags'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbtD0Hx8U99c",
        "colab_type": "text"
      },
      "source": [
        "__ Selecting 500 Tags __"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_nMDxAIU99d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "questions_explained = []\n",
        "total_tags=multilabel_y.shape[1]\n",
        "total_qs=preprocessed_data.shape[0]\n",
        "for i in range(500, total_tags, 100):\n",
        "    questions_explained.append(np.round(((total_qs-questions_explained_fn(i))/total_qs)*100,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fggMk2IJU99f",
        "colab_type": "code",
        "outputId": "52c4c546-d598-45ae-c288-5ff0f0db8e5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(questions_explained)\n",
        "xlabel = list(500+np.array(range(-50,450,50))*50)\n",
        "ax.set_xticklabels(xlabel)\n",
        "plt.xlabel(\"Number of tags\")\n",
        "plt.ylabel(\"Number Questions coverd partially\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "# you can choose any number of tags based on your computing power, minimun is 500(it covers 90% of the tags)\n",
        "print(\"with \",5500,\"tags we are covering \",questions_explained[50],\"% of questions\")\n",
        "print(\"with \",500,\"tags we are covering \",questions_explained[0],\"% of questions\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEGCAYAAACQO2mwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "with  5500 tags we are covering  99.481 % of questions\n",
            "with  500 tags we are covering  92.5 % of questions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuJzfmNrU99i",
        "colab_type": "code",
        "outputId": "46085a77-e124-4a5e-b71d-053964734dff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# we will be taking 500 tags\n",
        "multilabel_yx = tags_to_choose(500)\n",
        "print(\"number of questions that are not covered :\", questions_explained_fn(500),\"out of \", total_qs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of questions that are not covered : 7500 out of  100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsduwXTeU99k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_size=preprocessed_data.shape[0]\n",
        "train_size=int(0.80*total_size)\n",
        "\n",
        "x_train=preprocessed_data.head(train_size)\n",
        "x_test=preprocessed_data.tail(total_size - train_size)\n",
        "\n",
        "y_train = multilabel_yx[0:train_size,:]\n",
        "y_test = multilabel_yx[train_size:total_size,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZZDSH_VU99m",
        "colab_type": "code",
        "outputId": "31f62d03-2321-49ca-fe3c-a3183580cf75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Number of data points in train data :\", y_train.shape)\n",
        "print(\"Number of data points in test data :\", y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of data points in train data : (80000, 500)\n",
            "Number of data points in test data : (20000, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "664323eyU98a",
        "colab_type": "text"
      },
      "source": [
        "<h2> Featurizing data </h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDJ2PvnzU99o",
        "colab_type": "text"
      },
      "source": [
        "<h3> Featurizing data with Count vectorizer(BOW) upto 4Grams </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "530e8tW9U99o",
        "colab_type": "code",
        "outputId": "273b0cbc-b256-4f48-c3dd-05cbc17ac738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "start = datetime.now()\n",
        "vectorizer = CountVectorizer(min_df=0.00009, max_features=100000,tokenizer = lambda x: x.split() , ngram_range=(1,4))\n",
        "x_train_multilabel = vectorizer.fit_transform(x_train['question'])\n",
        "x_test_multilabel = vectorizer.transform(x_test['question'])\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken to run this cell : 0:01:33.147802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9iDfzXIU99t",
        "colab_type": "code",
        "outputId": "d6e555ca-4fb5-4f71-bde6-1065efee03b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Dimensions of train data X:\",x_train_multilabel.shape, \"Y :\",y_train.shape)\n",
        "print(\"Dimensions of test data X:\",x_test_multilabel.shape,\"Y:\",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimensions of train data X: (80000, 100000) Y : (80000, 500)\n",
            "Dimensions of test data X: (20000, 100000) Y: (20000, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kmmnoy4XU99v",
        "colab_type": "text"
      },
      "source": [
        "<h3> Applying Logistic Regression with OneVsRest Classifier </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUZ-TPpANOJI",
        "colab_type": "code",
        "outputId": "9ddfc80b-fdb8-42fa-f01c-9de37ad92e72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cv_scores = []\n",
        "alpha = [10**-5,10**-3,10**-1,10**0,10**1,10**3]\n",
        "for i in alpha:\n",
        "  classifier = OneVsRestClassifier(SGDClassifier(loss='log',alpha = i, penalty='l1'))\n",
        "  scores = cross_val_score(classifier, x_train_multilabel, y_train, cv=3, scoring='f1_micro')\n",
        "  cv_scores.append(scores.mean())\n",
        "\n",
        "f_score = [x for x in cv_scores]\n",
        "# determining best alpha\n",
        "optimal_alpha = alpha[f_score.index(max(f_score))]\n",
        "print('\\nThe optimal value of alpha is :', optimal_alpha)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The optimal value of alpha is : 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnHoxl5DU99w",
        "colab_type": "code",
        "outputId": "3a1c3286-3f14-4629-ab6a-3756196d70b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "start = datetime.now()\n",
        "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l1'))\n",
        "classifier.fit(x_train_multilabel, y_train)\n",
        "predictions = classifier.predict (x_test_multilabel)\n",
        "\n",
        "\n",
        "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
        "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
        "\n",
        "\n",
        "precision = precision_score(y_test, predictions, average='micro')\n",
        "recall = recall_score(y_test, predictions, average='micro')\n",
        "f1 = f1_score(y_test, predictions, average='micro')\n",
        " \n",
        "print(\"Micro-average quality numbers\")\n",
        "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
        "\n",
        "precision = precision_score(y_test, predictions, average='macro')\n",
        "recall = recall_score(y_test, predictions, average='macro')\n",
        "f1 = f1_score(y_test, predictions, average='macro')\n",
        " \n",
        "print(\"Macro-average quality numbers\")\n",
        "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
        "\n",
        "print (metrics.classification_report(y_test, predictions))\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 0.13485\n",
            "Hamming loss  0.0036648\n",
            "Micro-average quality numbers\n",
            "Precision: 0.5203, Recall: 0.2821, F1-measure: 0.3658\n",
            "Macro-average quality numbers\n",
            "Precision: 0.3418, Recall: 0.2176, F1-measure: 0.2409\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.29      0.41       820\n",
            "           1       0.41      0.13      0.19      1931\n",
            "           2       0.41      0.12      0.19       544\n",
            "           3       0.44      0.27      0.34       222\n",
            "           4       0.80      0.41      0.55      1311\n",
            "           5       0.68      0.56      0.61      1014\n",
            "           6       0.68      0.41      0.51      1374\n",
            "           7       0.74      0.60      0.66       702\n",
            "           8       0.80      0.58      0.67      1424\n",
            "           9       0.70      0.19      0.29      1037\n",
            "          10       0.70      0.40      0.51       797\n",
            "          11       0.53      0.42      0.47       156\n",
            "          12       0.49      0.47      0.48        36\n",
            "          13       0.74      0.47      0.57       610\n",
            "          14       0.34      0.19      0.24       405\n",
            "          15       0.65      0.24      0.35       144\n",
            "          16       0.46      0.27      0.34       425\n",
            "          17       0.59      0.17      0.27       485\n",
            "          18       0.67      0.47      0.55       269\n",
            "          19       0.79      0.64      0.71       518\n",
            "          20       0.60      0.12      0.20       529\n",
            "          21       0.62      0.61      0.62       294\n",
            "          22       0.85      0.44      0.58       520\n",
            "          23       0.47      0.28      0.35       246\n",
            "          24       0.56      0.11      0.18       312\n",
            "          25       0.40      0.25      0.30       314\n",
            "          26       0.67      0.34      0.45       190\n",
            "          27       0.27      0.11      0.15       342\n",
            "          28       0.30      0.26      0.28        96\n",
            "          29       0.38      0.19      0.25        32\n",
            "          30       0.07      0.04      0.05       747\n",
            "          31       1.00      0.43      0.60        14\n",
            "          32       0.59      0.60      0.60       166\n",
            "          33       0.43      0.40      0.41       171\n",
            "          34       0.71      0.23      0.34       256\n",
            "          35       0.67      0.58      0.62       199\n",
            "          36       0.00      0.00      0.00        60\n",
            "          37       0.28      0.27      0.28       203\n",
            "          38       0.66      0.47      0.55       201\n",
            "          39       0.19      0.24      0.21       208\n",
            "          40       0.33      0.38      0.36        13\n",
            "          41       0.55      0.22      0.31       154\n",
            "          42       0.39      0.22      0.28        69\n",
            "          43       0.00      0.00      0.00       426\n",
            "          44       0.40      0.44      0.42        77\n",
            "          45       0.32      0.36      0.34       223\n",
            "          46       0.42      0.24      0.30       144\n",
            "          47       0.82      0.51      0.63       245\n",
            "          48       0.38      0.20      0.26        91\n",
            "          49       0.56      0.18      0.27       157\n",
            "          50       0.79      0.82      0.81       132\n",
            "          51       0.67      0.78      0.72        41\n",
            "          52       0.44      0.52      0.48       124\n",
            "          53       0.21      0.17      0.19        96\n",
            "          54       0.30      0.09      0.14       128\n",
            "          55       0.58      0.24      0.34        46\n",
            "          56       0.65      0.60      0.62       151\n",
            "          57       0.11      0.03      0.04        80\n",
            "          58       0.35      0.18      0.24        65\n",
            "          59       0.34      0.29      0.31       182\n",
            "          60       0.94      0.72      0.82       148\n",
            "          61       0.22      0.09      0.13       196\n",
            "          62       0.41      0.12      0.19        58\n",
            "          63       0.75      0.35      0.48        43\n",
            "          64       0.68      0.37      0.48       197\n",
            "          65       0.65      0.38      0.48        82\n",
            "          66       0.57      0.08      0.14        50\n",
            "          67       0.51      0.50      0.50       105\n",
            "          68       0.10      0.11      0.11        98\n",
            "          69       0.16      0.10      0.12       238\n",
            "          70       0.14      0.09      0.11        35\n",
            "          71       0.26      0.30      0.28        54\n",
            "          72       0.05      0.12      0.07        25\n",
            "          73       0.09      0.10      0.09        29\n",
            "          74       0.00      0.00      0.00        29\n",
            "          75       0.24      0.17      0.20        40\n",
            "          76       0.54      0.68      0.60       105\n",
            "          77       0.29      0.43      0.35        28\n",
            "          78       0.16      0.02      0.04       202\n",
            "          79       0.48      0.57      0.52        37\n",
            "          80       0.43      0.20      0.27        15\n",
            "          81       0.32      0.46      0.38        52\n",
            "          82       0.30      0.16      0.21        50\n",
            "          83       0.12      0.05      0.07        56\n",
            "          84       0.46      0.43      0.44        54\n",
            "          85       0.47      0.41      0.44        34\n",
            "          86       0.20      0.07      0.10        30\n",
            "          87       0.20      0.21      0.20        29\n",
            "          88       0.44      0.46      0.45        24\n",
            "          89       0.69      0.80      0.74       117\n",
            "          90       0.26      0.17      0.20        66\n",
            "          91       0.34      0.19      0.25        68\n",
            "          92       0.79      0.22      0.35        67\n",
            "          93       0.20      0.43      0.27        28\n",
            "          94       0.36      0.24      0.29        17\n",
            "          95       0.64      0.59      0.61        51\n",
            "          96       0.57      0.47      0.52        53\n",
            "          97       1.00      0.05      0.09        61\n",
            "          98       0.00      0.00      0.00        79\n",
            "          99       0.36      0.28      0.31        18\n",
            "         100       0.25      0.18      0.21        11\n",
            "         101       0.71      0.38      0.50       207\n",
            "         102       0.00      0.00      0.00         6\n",
            "         103       0.25      0.07      0.11        30\n",
            "         104       0.17      0.04      0.06        54\n",
            "         105       0.81      0.33      0.47        39\n",
            "         106       0.27      0.23      0.25        70\n",
            "         107       0.75      0.21      0.33        14\n",
            "         108       0.35      0.33      0.34        66\n",
            "         109       0.47      0.38      0.42        50\n",
            "         110       0.86      0.07      0.13        87\n",
            "         111       0.38      0.49      0.43        51\n",
            "         112       0.00      0.00      0.00       291\n",
            "         113       0.88      0.76      0.81        49\n",
            "         114       0.08      0.03      0.04       110\n",
            "         115       0.11      0.07      0.09        28\n",
            "         116       0.00      0.00      0.00         5\n",
            "         117       0.24      0.07      0.11        56\n",
            "         118       0.78      0.40      0.53       125\n",
            "         119       0.46      0.27      0.34        44\n",
            "         120       0.83      0.12      0.21        42\n",
            "         121       0.32      0.13      0.18        55\n",
            "         122       0.72      0.57      0.64        68\n",
            "         123       0.12      0.05      0.07        82\n",
            "         124       0.00      0.00      0.00         0\n",
            "         125       1.00      0.71      0.83         7\n",
            "         126       0.12      0.22      0.16        18\n",
            "         127       0.35      0.23      0.27        31\n",
            "         128       0.78      0.54      0.64        13\n",
            "         129       0.58      0.52      0.55        50\n",
            "         130       0.12      0.01      0.02        91\n",
            "         131       0.49      0.60      0.54        35\n",
            "         132       0.08      0.12      0.10        26\n",
            "         133       0.00      0.00      0.00        32\n",
            "         134       0.56      0.26      0.35        35\n",
            "         135       0.66      0.68      0.67        37\n",
            "         136       0.00      0.00      0.00        55\n",
            "         137       0.21      0.37      0.27        41\n",
            "         138       0.00      0.00      0.00        15\n",
            "         139       0.20      0.10      0.14        99\n",
            "         140       0.92      0.57      0.71        86\n",
            "         141       0.50      0.15      0.23        53\n",
            "         142       0.29      0.17      0.21        36\n",
            "         143       0.41      0.52      0.46        66\n",
            "         144       0.71      0.31      0.43        64\n",
            "         145       0.00      0.00      0.00        25\n",
            "         146       0.10      0.04      0.06       125\n",
            "         147       0.26      0.40      0.32        15\n",
            "         148       0.72      0.58      0.64        48\n",
            "         149       0.23      0.20      0.21        65\n",
            "         150       0.00      0.00      0.00        11\n",
            "         151       0.31      0.60      0.41        15\n",
            "         152       0.17      0.08      0.11        52\n",
            "         153       0.25      0.11      0.15        18\n",
            "         154       0.14      0.06      0.09        16\n",
            "         155       0.00      0.00      0.00        20\n",
            "         156       0.30      0.07      0.12       121\n",
            "         157       0.48      0.33      0.39       107\n",
            "         158       0.33      0.13      0.19        15\n",
            "         159       0.73      0.36      0.48       105\n",
            "         160       0.41      0.20      0.27        69\n",
            "         161       0.33      0.14      0.20        56\n",
            "         162       0.20      0.02      0.04        47\n",
            "         163       0.17      0.05      0.08       121\n",
            "         164       0.42      0.34      0.38        41\n",
            "         165       0.00      0.00      0.00       229\n",
            "         166       0.50      0.07      0.12        98\n",
            "         167       0.75      0.09      0.16        33\n",
            "         168       0.19      0.09      0.12        44\n",
            "         169       0.48      0.44      0.46        45\n",
            "         170       0.83      0.39      0.53        51\n",
            "         171       0.00      0.00      0.00        18\n",
            "         172       0.74      0.48      0.58        48\n",
            "         173       0.00      0.00      0.00        12\n",
            "         174       0.21      0.23      0.22        62\n",
            "         175       0.43      0.68      0.53        44\n",
            "         176       0.66      0.83      0.74        30\n",
            "         177       0.53      0.33      0.41        30\n",
            "         178       0.00      0.00      0.00         0\n",
            "         179       0.02      1.00      0.04         1\n",
            "         180       0.40      0.20      0.27        40\n",
            "         181       0.14      0.07      0.09        44\n",
            "         182       1.00      0.50      0.67         2\n",
            "         183       0.27      0.08      0.12        75\n",
            "         184       0.25      0.25      0.25         4\n",
            "         185       0.67      0.31      0.43        64\n",
            "         186       0.31      0.33      0.32        12\n",
            "         187       0.97      0.53      0.68        55\n",
            "         188       0.77      0.69      0.73        64\n",
            "         189       0.10      0.04      0.06        96\n",
            "         190       0.00      0.00      0.00        22\n",
            "         191       0.92      0.16      0.27        76\n",
            "         192       0.37      0.40      0.38        45\n",
            "         193       0.40      0.14      0.21        14\n",
            "         194       0.52      0.60      0.56        50\n",
            "         195       0.08      0.10      0.09        20\n",
            "         196       0.77      0.66      0.71        35\n",
            "         197       0.47      0.16      0.24        94\n",
            "         198       0.00      0.00      0.00        14\n",
            "         199       0.00      0.00      0.00        25\n",
            "         200       0.00      0.00      0.00        54\n",
            "         201       0.50      0.14      0.21        22\n",
            "         202       0.20      0.02      0.04        43\n",
            "         203       0.00      0.00      0.00        43\n",
            "         204       0.93      0.23      0.36        62\n",
            "         205       0.00      0.00      0.00         3\n",
            "         206       0.26      0.19      0.22        43\n",
            "         207       0.08      0.14      0.11         7\n",
            "         208       0.11      0.25      0.15         8\n",
            "         209       0.09      0.17      0.11        42\n",
            "         210       0.28      0.50      0.36        10\n",
            "         211       0.21      0.23      0.22        40\n",
            "         212       0.50      0.26      0.34        23\n",
            "         213       0.00      0.00      0.00         6\n",
            "         214       0.67      0.38      0.49        47\n",
            "         215       0.75      0.10      0.17        62\n",
            "         216       0.50      0.06      0.11        77\n",
            "         217       0.50      0.18      0.27        22\n",
            "         218       0.00      0.00      0.00         3\n",
            "         219       0.10      0.04      0.05        28\n",
            "         220       0.33      0.17      0.23        81\n",
            "         221       0.20      0.03      0.06        31\n",
            "         222       0.00      0.00      0.00        34\n",
            "         223       1.00      0.18      0.31        60\n",
            "         224       0.00      0.00      0.00        10\n",
            "         225       0.80      0.40      0.53        10\n",
            "         226       0.75      0.61      0.67        92\n",
            "         227       0.50      0.15      0.24        13\n",
            "         228       0.12      0.08      0.10        13\n",
            "         229       0.85      0.65      0.74        43\n",
            "         230       0.41      0.26      0.32        35\n",
            "         231       0.25      0.25      0.25         4\n",
            "         232       0.37      0.35      0.36        20\n",
            "         233       0.00      0.00      0.00       145\n",
            "         234       0.70      0.56      0.63        55\n",
            "         235       0.00      0.00      0.00         2\n",
            "         236       0.12      0.11      0.12        37\n",
            "         237       0.76      0.43      0.55        90\n",
            "         238       0.00      0.00      0.00        58\n",
            "         239       0.50      0.35      0.41        20\n",
            "         240       0.93      0.46      0.62        61\n",
            "         241       0.81      0.83      0.82        42\n",
            "         242       0.47      0.87      0.61        30\n",
            "         243       0.81      0.44      0.57        66\n",
            "         244       0.45      0.21      0.29        42\n",
            "         245       0.00      0.00      0.00        31\n",
            "         246       0.05      0.33      0.09         6\n",
            "         247       0.09      0.06      0.07        18\n",
            "         248       0.79      0.53      0.64        51\n",
            "         249       0.86      0.35      0.50        17\n",
            "         250       0.56      0.23      0.32        22\n",
            "         251       0.65      0.54      0.59        52\n",
            "         252       0.30      0.10      0.15        29\n",
            "         253       0.11      0.07      0.09        28\n",
            "         254       0.00      0.00      0.00        10\n",
            "         255       0.04      0.20      0.07         5\n",
            "         256       0.20      0.33      0.25         3\n",
            "         257       0.39      0.17      0.24        41\n",
            "         258       0.23      0.10      0.14        30\n",
            "         259       1.00      0.33      0.50         3\n",
            "         260       0.00      0.00      0.00        38\n",
            "         261       0.00      0.00      0.00         1\n",
            "         262       0.43      0.47      0.45        19\n",
            "         263       0.00      0.00      0.00        14\n",
            "         264       0.00      0.00      0.00        37\n",
            "         265       0.00      0.00      0.00         9\n",
            "         266       0.15      0.24      0.18        45\n",
            "         267       0.47      0.67      0.55        33\n",
            "         268       0.72      0.81      0.76        16\n",
            "         269       0.41      0.37      0.39        35\n",
            "         270       0.50      0.27      0.35        11\n",
            "         271       0.00      0.00      0.00        30\n",
            "         272       0.27      0.38      0.32         8\n",
            "         273       0.03      0.05      0.04        21\n",
            "         274       0.00      0.00      0.00       123\n",
            "         275       0.54      0.10      0.18        67\n",
            "         276       0.77      0.85      0.81        20\n",
            "         277       0.04      0.07      0.05        14\n",
            "         278       0.00      0.00      0.00        19\n",
            "         279       0.37      0.58      0.45        12\n",
            "         280       0.05      0.13      0.08        15\n",
            "         281       0.77      0.59      0.67        17\n",
            "         282       0.97      0.76      0.85        41\n",
            "         283       0.00      0.00      0.00        15\n",
            "         284       0.00      0.00      0.00        74\n",
            "         285       0.33      0.11      0.16        38\n",
            "         286       0.33      0.25      0.29        16\n",
            "         287       0.10      0.13      0.12        30\n",
            "         288       0.90      0.32      0.47        28\n",
            "         289       0.00      0.00      0.00        21\n",
            "         290       0.78      0.44      0.56        41\n",
            "         291       0.00      0.00      0.00        12\n",
            "         292       0.00      0.00      0.00        24\n",
            "         293       0.27      0.15      0.19        20\n",
            "         294       0.14      0.13      0.13        23\n",
            "         295       0.17      0.03      0.06        29\n",
            "         296       0.17      0.07      0.10        28\n",
            "         297       0.00      0.00      0.00        42\n",
            "         298       0.00      0.00      0.00        53\n",
            "         299       0.23      0.08      0.12        36\n",
            "         300       0.26      0.12      0.17        41\n",
            "         301       0.50      0.22      0.30        37\n",
            "         302       0.90      0.35      0.50        26\n",
            "         303       0.00      0.00      0.00        11\n",
            "         304       0.00      0.00      0.00        31\n",
            "         305       0.50      0.29      0.37        17\n",
            "         306       0.12      0.11      0.12         9\n",
            "         307       1.00      0.17      0.29         6\n",
            "         308       0.00      0.00      0.00        34\n",
            "         309       0.62      0.23      0.34        43\n",
            "         310       0.00      0.00      0.00        30\n",
            "         311       0.25      0.16      0.20        50\n",
            "         312       0.00      0.00      0.00        24\n",
            "         313       0.00      0.00      0.00        42\n",
            "         314       0.50      0.23      0.31        22\n",
            "         315       0.00      0.00      0.00        58\n",
            "         316       0.67      0.20      0.31        10\n",
            "         317       0.33      0.25      0.28        57\n",
            "         318       0.00      0.00      0.00        10\n",
            "         319       0.00      0.00      0.00        11\n",
            "         320       0.00      0.00      0.00        11\n",
            "         321       0.38      0.38      0.38         8\n",
            "         322       0.80      0.36      0.50        22\n",
            "         323       0.94      0.57      0.71        28\n",
            "         324       0.48      0.52      0.50        50\n",
            "         325       0.29      0.11      0.16        18\n",
            "         326       0.00      0.00      0.00        33\n",
            "         327       0.12      0.06      0.08        17\n",
            "         328       0.33      0.03      0.06        29\n",
            "         329       0.00      0.00      0.00         7\n",
            "         330       0.50      0.40      0.44        10\n",
            "         331       0.12      0.04      0.06        25\n",
            "         332       0.67      1.00      0.80         2\n",
            "         333       0.27      0.27      0.27        11\n",
            "         334       0.00      0.00      0.00        24\n",
            "         335       1.00      0.20      0.33         5\n",
            "         336       0.00      0.00      0.00        33\n",
            "         337       0.40      0.07      0.11        30\n",
            "         338       0.94      0.38      0.54        42\n",
            "         339       0.08      0.04      0.05        26\n",
            "         340       0.33      0.50      0.40        36\n",
            "         341       1.00      0.46      0.63        13\n",
            "         342       0.00      0.00      0.00        11\n",
            "         343       0.00      0.00      0.00        10\n",
            "         344       0.11      0.10      0.10        21\n",
            "         345       0.00      0.00      0.00         0\n",
            "         346       0.00      0.00      0.00         6\n",
            "         347       0.20      0.08      0.12        12\n",
            "         348       0.33      0.15      0.21        13\n",
            "         349       0.25      0.04      0.07        24\n",
            "         350       0.69      0.33      0.45        27\n",
            "         351       0.28      0.12      0.16        43\n",
            "         352       0.00      0.00      0.00        30\n",
            "         353       0.45      0.41      0.43        22\n",
            "         354       0.10      0.03      0.05        31\n",
            "         355       0.75      0.30      0.43        10\n",
            "         356       0.12      0.05      0.07        20\n",
            "         357       0.68      0.75      0.71        20\n",
            "         358       0.29      0.25      0.27        28\n",
            "         359       0.26      0.52      0.35        21\n",
            "         360       0.00      0.00      0.00        25\n",
            "         361       0.53      0.23      0.32        35\n",
            "         362       0.86      0.17      0.28        36\n",
            "         363       0.33      0.12      0.17        17\n",
            "         364       1.00      0.08      0.14        13\n",
            "         365       0.00      0.00      0.00        21\n",
            "         366       0.00      0.00      0.00        18\n",
            "         367       0.00      0.00      0.00        97\n",
            "         368       0.38      0.21      0.27        29\n",
            "         369       1.00      0.08      0.15        12\n",
            "         370       0.03      0.08      0.05        13\n",
            "         371       0.17      0.11      0.13        18\n",
            "         372       0.00      0.00      0.00         6\n",
            "         373       0.20      0.17      0.18         6\n",
            "         374       0.25      0.03      0.06        30\n",
            "         375       0.21      0.11      0.15        27\n",
            "         376       0.00      0.00      0.00        28\n",
            "         377       0.00      0.00      0.00         2\n",
            "         378       0.50      0.25      0.33         4\n",
            "         379       0.12      0.05      0.07        19\n",
            "         380       0.11      0.40      0.17         5\n",
            "         381       0.00      0.00      0.00        18\n",
            "         382       0.33      0.23      0.27        22\n",
            "         383       0.00      0.00      0.00        16\n",
            "         384       0.33      0.38      0.36        13\n",
            "         385       0.11      0.28      0.16        18\n",
            "         386       0.83      0.45      0.59        11\n",
            "         387       0.00      0.00      0.00        88\n",
            "         388       0.00      0.00      0.00        13\n",
            "         389       0.00      0.00      0.00         6\n",
            "         390       0.00      0.00      0.00         6\n",
            "         391       1.00      0.06      0.11        51\n",
            "         392       0.00      0.00      0.00        13\n",
            "         393       0.80      0.11      0.19        37\n",
            "         394       0.00      0.00      0.00         6\n",
            "         395       0.00      0.00      0.00         9\n",
            "         396       0.25      0.08      0.12        13\n",
            "         397       0.67      0.33      0.44         6\n",
            "         398       0.53      0.28      0.36        29\n",
            "         399       0.94      0.48      0.64        33\n",
            "         400       0.00      0.00      0.00        31\n",
            "         401       0.46      0.12      0.19        50\n",
            "         402       1.00      0.44      0.62        18\n",
            "         403       0.50      0.14      0.22         7\n",
            "         404       0.67      0.46      0.55        26\n",
            "         405       0.00      0.00      0.00        56\n",
            "         406       1.00      0.50      0.67         4\n",
            "         407       0.10      0.06      0.07        17\n",
            "         408       0.00      0.00      0.00        11\n",
            "         409       0.00      0.00      0.00        18\n",
            "         410       0.25      0.40      0.31        10\n",
            "         411       0.14      0.04      0.07        45\n",
            "         412       0.56      0.25      0.34        20\n",
            "         413       0.00      0.00      0.00        25\n",
            "         414       0.00      0.00      0.00        20\n",
            "         415       0.25      0.33      0.29         6\n",
            "         416       0.29      0.08      0.12        26\n",
            "         417       0.00      0.00      0.00        10\n",
            "         418       0.00      0.00      0.00        18\n",
            "         419       0.67      0.67      0.67         6\n",
            "         420       0.73      0.47      0.57        17\n",
            "         421       0.08      1.00      0.14         1\n",
            "         422       0.00      0.00      0.00         6\n",
            "         423       0.00      0.00      0.00        12\n",
            "         424       0.00      0.00      0.00         4\n",
            "         425       0.50      0.09      0.15        11\n",
            "         426       0.00      0.00      0.00        11\n",
            "         427       0.86      0.75      0.80         8\n",
            "         428       0.38      0.12      0.18        26\n",
            "         429       0.46      0.42      0.44        40\n",
            "         430       0.00      0.00      0.00         2\n",
            "         431       0.00      0.00      0.00        35\n",
            "         432       0.00      0.00      0.00        15\n",
            "         433       0.00      0.00      0.00        18\n",
            "         434       0.00      0.00      0.00         0\n",
            "         435       0.00      0.00      0.00         0\n",
            "         436       0.10      0.18      0.13        28\n",
            "         437       0.33      0.15      0.21        33\n",
            "         438       1.00      0.35      0.52        20\n",
            "         439       0.00      0.00      0.00        36\n",
            "         440       0.00      0.00      0.00        18\n",
            "         441       0.54      0.39      0.45        18\n",
            "         442       0.69      0.56      0.62        16\n",
            "         443       0.05      0.09      0.07        22\n",
            "         444       0.00      0.00      0.00         6\n",
            "         445       1.00      0.29      0.44        21\n",
            "         446       0.86      0.39      0.54        46\n",
            "         447       0.00      0.00      0.00        69\n",
            "         448       0.00      0.00      0.00         7\n",
            "         449       0.11      0.33      0.17         3\n",
            "         450       0.00      0.00      0.00        52\n",
            "         451       0.00      0.00      0.00        16\n",
            "         452       1.00      0.53      0.69        17\n",
            "         453       0.00      0.00      0.00        13\n",
            "         454       0.57      0.36      0.44        11\n",
            "         455       0.00      0.00      0.00        12\n",
            "         456       0.00      0.00      0.00         6\n",
            "         457       0.00      0.00      0.00        18\n",
            "         458       0.00      0.00      0.00        15\n",
            "         459       0.00      0.00      0.00        28\n",
            "         460       0.00      0.00      0.00        18\n",
            "         461       0.25      0.10      0.14        10\n",
            "         462       0.33      0.04      0.07        24\n",
            "         463       0.00      0.00      0.00        18\n",
            "         464       1.00      0.18      0.30        39\n",
            "         465       0.12      0.09      0.11        11\n",
            "         466       0.09      0.03      0.04        35\n",
            "         467       0.15      0.19      0.17        21\n",
            "         468       0.40      0.11      0.17        37\n",
            "         469       0.00      0.00      0.00         5\n",
            "         470       0.00      0.00      0.00         8\n",
            "         471       0.73      0.30      0.42        37\n",
            "         472       0.00      0.00      0.00        47\n",
            "         473       1.00      0.07      0.13        14\n",
            "         474       1.00      0.43      0.61        23\n",
            "         475       0.70      0.11      0.18        66\n",
            "         476       0.00      0.00      0.00         3\n",
            "         477       0.42      0.26      0.32        19\n",
            "         478       0.00      0.00      0.00         1\n",
            "         479       0.00      0.00      0.00        23\n",
            "         480       0.00      0.00      0.00        60\n",
            "         481       0.00      0.00      0.00        26\n",
            "         482       0.50      0.25      0.33         4\n",
            "         483       0.00      0.00      0.00         8\n",
            "         484       0.54      0.30      0.39        23\n",
            "         485       0.42      0.28      0.33        18\n",
            "         486       0.25      0.08      0.12        12\n",
            "         487       0.20      0.07      0.10        29\n",
            "         488       0.25      1.00      0.40         1\n",
            "         489       0.44      0.67      0.53         6\n",
            "         490       0.22      0.29      0.25         7\n",
            "         491       0.00      0.00      0.00         3\n",
            "         492       0.25      0.10      0.14        10\n",
            "         493       0.26      0.58      0.36        19\n",
            "         494       0.00      0.00      0.00         7\n",
            "         495       0.50      0.12      0.20         8\n",
            "         496       0.22      0.11      0.15        18\n",
            "         497       0.00      0.00      0.00        72\n",
            "         498       0.00      0.00      0.00         8\n",
            "         499       0.00      0.00      0.00        32\n",
            "\n",
            "   micro avg       0.52      0.28      0.37     37472\n",
            "   macro avg       0.34      0.22      0.24     37472\n",
            "weighted avg       0.49      0.28      0.34     37472\n",
            " samples avg       0.35      0.28      0.29     37472\n",
            "\n",
            "Time taken to run this cell : 0:12:38.171004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRDtt5kK8Fx4",
        "colab_type": "text"
      },
      "source": [
        "<h3> Applying Linear SVM with OneVsRest Classifier </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7bpzLho8JXP",
        "colab_type": "code",
        "outputId": "7fdbb0ba-fd90-4633-906a-80d3f858a547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "start = datetime.now()\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cv_scores = []\n",
        "alpha = [10**-5,10**-3,10**-1,10**1,10**3]\n",
        "for i in alpha:\n",
        "  classifier = OneVsRestClassifier(SGDClassifier(loss='hinge',alpha = i, penalty='l1'))\n",
        "  scores = cross_val_score(classifier, x_train_multilabel, y_train, cv=3, scoring='f1_micro')\n",
        "  print(\"for alpha :\", alpha ,\"f1 score is :\" ,scores.mean())\n",
        "  cv_scores.append(scores.mean())\n",
        "\n",
        "f_score = [x for x in cv_scores]\n",
        "# determining best alpha\n",
        "optimal_alpha = alpha[f_score.index(max(f_score))]\n",
        "print('\\nThe optimal value of alpha is :', optimal_alpha)\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for alpha : [1e-05, 0.001, 0.1, 10, 1000] f1 score is : 0.28895533647313343\n",
            "for alpha : [1e-05, 0.001, 0.1, 10, 1000] f1 score is : 0.3803564728946876\n",
            "for alpha : [1e-05, 0.001, 0.1, 10, 1000] f1 score is : 0.017655385937113577\n",
            "for alpha : [1e-05, 0.001, 0.1, 10, 1000] f1 score is : 0.0\n",
            "for alpha : [1e-05, 0.001, 0.1, 10, 1000] f1 score is : 0.0039678177991261596\n",
            "\n",
            "The optimal value of alpha is : 0.001\n",
            "Time taken to run this cell : 2:18:13.812353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAS8vg_efHNL",
        "colab_type": "text"
      },
      "source": [
        " In the above cell there is a typing mistake, editing and running it againa takes a lot of time so iam correcting here.\n",
        " - for alpha : 1e-05, f1 score is : 0.28895533647313343\n",
        " - for alpha :  0.001, f1 score is : 0.3803564728946876\n",
        " - for alpha :  0.1,  f1 score is : 0.017655385937113577\n",
        " - for alpha :  10,  f1 score is : 0.0\n",
        " - for alpha :  1000,  f1 score is : 0.0039678177991261596\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO1yaq1yf0IY",
        "colab_type": "code",
        "outputId": "da19dee2-e436-4d5f-a028-29e2b0e16dac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "start = datetime.now()\n",
        "classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=0.001, penalty='l1'))\n",
        "classifier.fit(x_train_multilabel, y_train)\n",
        "predictions = classifier.predict (x_test_multilabel)\n",
        "\n",
        "\n",
        "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
        "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
        "\n",
        "\n",
        "precision = precision_score(y_test, predictions, average='micro')\n",
        "recall = recall_score(y_test, predictions, average='micro')\n",
        "f1 = f1_score(y_test, predictions, average='micro')\n",
        " \n",
        "print(\"Micro-average quality numbers\")\n",
        "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
        "\n",
        "precision = precision_score(y_test, predictions, average='macro')\n",
        "recall = recall_score(y_test, predictions, average='macro')\n",
        "f1 = f1_score(y_test, predictions, average='macro')\n",
        " \n",
        "print(\"Macro-average quality numbers\")\n",
        "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
        "\n",
        "print (metrics.classification_report(y_test, predictions))\n",
        "print(\"Time taken to run this cell :\", datetime.now() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 0.13425\n",
            "Hamming loss  0.0036292\n",
            "Micro-average quality numbers\n",
            "Precision: 0.5313, Recall: 0.2676, F1-measure: 0.3560\n",
            "Macro-average quality numbers\n",
            "Precision: 0.2835, Recall: 0.2098, F1-measure: 0.2196\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.33      0.43       820\n",
            "           1       0.27      0.04      0.07      1931\n",
            "           2       0.44      0.15      0.23       544\n",
            "           3       0.43      0.28      0.34       222\n",
            "           4       0.83      0.40      0.54      1311\n",
            "           5       0.75      0.53      0.62      1014\n",
            "           6       0.69      0.41      0.51      1374\n",
            "           7       0.80      0.58      0.67       702\n",
            "           8       0.91      0.55      0.68      1424\n",
            "           9       0.66      0.29      0.40      1037\n",
            "          10       0.73      0.40      0.52       797\n",
            "          11       0.57      0.31      0.40       156\n",
            "          12       0.35      0.47      0.40        36\n",
            "          13       0.77      0.41      0.54       610\n",
            "          14       0.11      0.00      0.00       405\n",
            "          15       0.50      0.28      0.36       144\n",
            "          16       0.68      0.20      0.31       425\n",
            "          17       0.38      0.12      0.18       485\n",
            "          18       0.43      0.51      0.47       269\n",
            "          19       0.77      0.64      0.70       518\n",
            "          20       0.57      0.13      0.21       529\n",
            "          21       0.73      0.64      0.68       294\n",
            "          22       0.83      0.46      0.59       520\n",
            "          23       0.57      0.24      0.34       246\n",
            "          24       0.39      0.20      0.27       312\n",
            "          25       0.36      0.27      0.31       314\n",
            "          26       0.47      0.30      0.37       190\n",
            "          27       0.00      0.00      0.00       342\n",
            "          28       0.19      0.20      0.19        96\n",
            "          29       0.20      0.19      0.19        32\n",
            "          30       0.38      0.08      0.14       747\n",
            "          31       0.23      0.50      0.31        14\n",
            "          32       0.42      0.58      0.49       166\n",
            "          33       0.57      0.35      0.43       171\n",
            "          34       0.62      0.32      0.42       256\n",
            "          35       0.66      0.62      0.64       199\n",
            "          36       0.06      0.07      0.06        60\n",
            "          37       0.00      0.00      0.00       203\n",
            "          38       0.50      0.53      0.52       201\n",
            "          39       0.03      0.00      0.01       208\n",
            "          40       0.50      0.46      0.48        13\n",
            "          41       0.40      0.20      0.27       154\n",
            "          42       0.36      0.33      0.35        69\n",
            "          43       0.04      0.01      0.01       426\n",
            "          44       0.41      0.22      0.29        77\n",
            "          45       0.14      0.00      0.01       223\n",
            "          46       0.34      0.30      0.32       144\n",
            "          47       0.76      0.46      0.57       245\n",
            "          48       0.71      0.05      0.10        91\n",
            "          49       0.61      0.17      0.27       157\n",
            "          50       0.66      0.76      0.71       132\n",
            "          51       0.72      0.83      0.77        41\n",
            "          52       0.54      0.36      0.43       124\n",
            "          53       0.10      0.20      0.14        96\n",
            "          54       0.08      0.07      0.08       128\n",
            "          55       0.41      0.52      0.46        46\n",
            "          56       0.58      0.73      0.65       151\n",
            "          57       0.00      0.00      0.00        80\n",
            "          58       0.21      0.17      0.19        65\n",
            "          59       0.00      0.00      0.00       182\n",
            "          60       0.72      0.67      0.69       148\n",
            "          61       0.18      0.09      0.12       196\n",
            "          62       0.00      0.00      0.00        58\n",
            "          63       0.62      0.35      0.45        43\n",
            "          64       0.62      0.35      0.45       197\n",
            "          65       0.51      0.29      0.37        82\n",
            "          66       0.08      0.02      0.03        50\n",
            "          67       0.45      0.57      0.50       105\n",
            "          68       0.00      0.00      0.00        98\n",
            "          69       0.02      0.00      0.01       238\n",
            "          70       0.27      0.11      0.16        35\n",
            "          71       0.34      0.22      0.27        54\n",
            "          72       0.25      0.16      0.20        25\n",
            "          73       1.00      0.03      0.07        29\n",
            "          74       0.00      0.00      0.00        29\n",
            "          75       0.50      0.17      0.26        40\n",
            "          76       0.68      0.67      0.67       105\n",
            "          77       0.20      0.04      0.06        28\n",
            "          78       0.54      0.03      0.07       202\n",
            "          79       0.67      0.11      0.19        37\n",
            "          80       0.12      0.33      0.18        15\n",
            "          81       0.25      0.40      0.31        52\n",
            "          82       0.24      0.20      0.22        50\n",
            "          83       0.00      0.00      0.00        56\n",
            "          84       0.62      0.57      0.60        54\n",
            "          85       0.32      0.50      0.39        34\n",
            "          86       0.44      0.13      0.21        30\n",
            "          87       0.30      0.21      0.24        29\n",
            "          88       0.43      0.50      0.46        24\n",
            "          89       0.82      0.80      0.81       117\n",
            "          90       0.11      0.14      0.12        66\n",
            "          91       0.00      0.00      0.00        68\n",
            "          92       0.64      0.27      0.38        67\n",
            "          93       0.21      0.21      0.21        28\n",
            "          94       0.17      0.35      0.23        17\n",
            "          95       0.71      0.69      0.70        51\n",
            "          96       0.58      0.34      0.43        53\n",
            "          97       0.00      0.00      0.00        61\n",
            "          98       0.00      0.00      0.00        79\n",
            "          99       0.15      0.33      0.21        18\n",
            "         100       0.21      0.27      0.24        11\n",
            "         101       0.66      0.47      0.55       207\n",
            "         102       0.00      0.00      0.00         6\n",
            "         103       0.29      0.07      0.11        30\n",
            "         104       0.00      0.00      0.00        54\n",
            "         105       0.54      0.49      0.51        39\n",
            "         106       0.21      0.19      0.20        70\n",
            "         107       0.43      0.21      0.29        14\n",
            "         108       0.48      0.38      0.42        66\n",
            "         109       0.28      0.22      0.25        50\n",
            "         110       0.55      0.07      0.12        87\n",
            "         111       0.50      0.10      0.16        51\n",
            "         112       0.00      0.00      0.00       291\n",
            "         113       0.69      0.71      0.70        49\n",
            "         114       0.00      0.00      0.00       110\n",
            "         115       0.08      0.04      0.05        28\n",
            "         116       0.00      0.00      0.00         5\n",
            "         117       0.00      0.00      0.00        56\n",
            "         118       0.77      0.55      0.64       125\n",
            "         119       0.50      0.39      0.44        44\n",
            "         120       0.75      0.14      0.24        42\n",
            "         121       0.27      0.29      0.28        55\n",
            "         122       0.62      0.54      0.58        68\n",
            "         123       0.00      0.00      0.00        82\n",
            "         124       0.00      0.00      0.00         0\n",
            "         125       0.36      0.57      0.44         7\n",
            "         126       0.06      0.11      0.08        18\n",
            "         127       0.00      0.00      0.00        31\n",
            "         128       0.54      0.54      0.54        13\n",
            "         129       0.51      0.50      0.51        50\n",
            "         130       0.01      0.01      0.01        91\n",
            "         131       0.46      0.63      0.53        35\n",
            "         132       0.00      0.00      0.00        26\n",
            "         133       0.00      0.00      0.00        32\n",
            "         134       0.50      0.11      0.19        35\n",
            "         135       0.71      0.65      0.68        37\n",
            "         136       0.00      0.00      0.00        55\n",
            "         137       0.10      0.20      0.14        41\n",
            "         138       0.00      0.00      0.00        15\n",
            "         139       0.00      0.00      0.00        99\n",
            "         140       0.91      0.62      0.74        86\n",
            "         141       0.00      0.00      0.00        53\n",
            "         142       0.67      0.11      0.19        36\n",
            "         143       0.46      0.52      0.49        66\n",
            "         144       0.77      0.47      0.58        64\n",
            "         145       0.00      0.00      0.00        25\n",
            "         146       0.04      0.01      0.01       125\n",
            "         147       0.38      0.40      0.39        15\n",
            "         148       0.61      0.42      0.49        48\n",
            "         149       0.00      0.00      0.00        65\n",
            "         150       0.00      0.00      0.00        11\n",
            "         151       0.28      0.53      0.36        15\n",
            "         152       0.00      0.00      0.00        52\n",
            "         153       0.11      0.11      0.11        18\n",
            "         154       0.00      0.00      0.00        16\n",
            "         155       0.80      0.20      0.32        20\n",
            "         156       0.29      0.07      0.12       121\n",
            "         157       0.38      0.47      0.42       107\n",
            "         158       0.25      0.07      0.11        15\n",
            "         159       0.63      0.57      0.60       105\n",
            "         160       0.33      0.26      0.29        69\n",
            "         161       0.00      0.00      0.00        56\n",
            "         162       0.33      0.02      0.04        47\n",
            "         163       0.00      0.00      0.00       121\n",
            "         164       0.36      0.22      0.27        41\n",
            "         165       0.00      0.00      0.00       229\n",
            "         166       0.75      0.06      0.11        98\n",
            "         167       0.75      0.09      0.16        33\n",
            "         168       0.67      0.18      0.29        44\n",
            "         169       0.43      0.64      0.52        45\n",
            "         170       0.83      0.47      0.60        51\n",
            "         171       0.00      0.00      0.00        18\n",
            "         172       0.36      0.54      0.43        48\n",
            "         173       0.00      0.00      0.00        12\n",
            "         174       0.00      0.00      0.00        62\n",
            "         175       0.62      0.86      0.72        44\n",
            "         176       0.83      0.67      0.74        30\n",
            "         177       0.21      0.27      0.23        30\n",
            "         178       0.00      0.00      0.00         0\n",
            "         179       1.00      1.00      1.00         1\n",
            "         180       0.47      0.53      0.49        40\n",
            "         181       0.11      0.14      0.12        44\n",
            "         182       0.08      0.50      0.14         2\n",
            "         183       0.19      0.19      0.19        75\n",
            "         184       0.00      0.00      0.00         4\n",
            "         185       0.53      0.28      0.37        64\n",
            "         186       0.29      0.33      0.31        12\n",
            "         187       0.75      0.16      0.27        55\n",
            "         188       0.66      0.62      0.64        64\n",
            "         189       0.00      0.00      0.00        96\n",
            "         190       0.00      0.00      0.00        22\n",
            "         191       0.48      0.29      0.36        76\n",
            "         192       0.94      0.36      0.52        45\n",
            "         193       0.33      0.21      0.26        14\n",
            "         194       0.53      0.54      0.53        50\n",
            "         195       0.56      0.25      0.34        20\n",
            "         196       0.74      0.66      0.70        35\n",
            "         197       0.00      0.00      0.00        94\n",
            "         198       0.00      0.00      0.00        14\n",
            "         199       0.00      0.00      0.00        25\n",
            "         200       0.00      0.00      0.00        54\n",
            "         201       0.25      0.05      0.08        22\n",
            "         202       0.00      0.00      0.00        43\n",
            "         203       0.00      0.00      0.00        43\n",
            "         204       0.84      0.42      0.56        62\n",
            "         205       0.00      0.00      0.00         3\n",
            "         206       0.07      0.09      0.08        43\n",
            "         207       0.00      0.00      0.00         7\n",
            "         208       0.03      0.12      0.04         8\n",
            "         209       0.00      0.00      0.00        42\n",
            "         210       0.33      0.60      0.43        10\n",
            "         211       0.23      0.12      0.16        40\n",
            "         212       0.38      0.22      0.28        23\n",
            "         213       0.00      0.00      0.00         6\n",
            "         214       0.53      0.55      0.54        47\n",
            "         215       0.00      0.00      0.00        62\n",
            "         216       0.31      0.16      0.21        77\n",
            "         217       0.45      0.23      0.30        22\n",
            "         218       0.00      0.00      0.00         3\n",
            "         219       0.00      0.00      0.00        28\n",
            "         220       0.50      0.02      0.05        81\n",
            "         221       0.00      0.00      0.00        31\n",
            "         222       0.00      0.00      0.00        34\n",
            "         223       0.95      0.33      0.49        60\n",
            "         224       0.02      0.10      0.03        10\n",
            "         225       0.53      0.80      0.64        10\n",
            "         226       0.73      0.67      0.70        92\n",
            "         227       0.50      0.38      0.43        13\n",
            "         228       0.33      0.08      0.12        13\n",
            "         229       0.62      0.47      0.53        43\n",
            "         230       0.00      0.00      0.00        35\n",
            "         231       0.00      0.00      0.00         4\n",
            "         232       0.35      0.45      0.39        20\n",
            "         233       0.00      0.00      0.00       145\n",
            "         234       0.53      0.69      0.60        55\n",
            "         235       0.00      0.00      0.00         2\n",
            "         236       0.21      0.19      0.20        37\n",
            "         237       0.69      0.52      0.59        90\n",
            "         238       0.00      0.00      0.00        58\n",
            "         239       0.50      0.15      0.23        20\n",
            "         240       0.91      0.51      0.65        61\n",
            "         241       0.59      0.79      0.67        42\n",
            "         242       0.40      0.57      0.47        30\n",
            "         243       0.58      0.68      0.63        66\n",
            "         244       0.40      0.33      0.36        42\n",
            "         245       0.00      0.00      0.00        31\n",
            "         246       1.00      0.50      0.67         6\n",
            "         247       0.00      0.00      0.00        18\n",
            "         248       0.76      0.57      0.65        51\n",
            "         249       0.75      0.35      0.48        17\n",
            "         250       0.64      0.41      0.50        22\n",
            "         251       0.70      0.27      0.39        52\n",
            "         252       0.08      0.03      0.05        29\n",
            "         253       0.11      0.04      0.05        28\n",
            "         254       0.00      0.00      0.00        10\n",
            "         255       0.03      0.20      0.05         5\n",
            "         256       0.17      0.67      0.27         3\n",
            "         257       0.20      0.29      0.24        41\n",
            "         258       0.07      0.10      0.08        30\n",
            "         259       0.50      0.67      0.57         3\n",
            "         260       0.00      0.00      0.00        38\n",
            "         261       0.00      0.00      0.00         1\n",
            "         262       0.46      0.32      0.37        19\n",
            "         263       0.00      0.00      0.00        14\n",
            "         264       0.00      0.00      0.00        37\n",
            "         265       0.00      0.00      0.00         9\n",
            "         266       0.20      0.20      0.20        45\n",
            "         267       0.43      0.70      0.53        33\n",
            "         268       0.63      0.75      0.69        16\n",
            "         269       0.48      0.57      0.52        35\n",
            "         270       0.25      0.27      0.26        11\n",
            "         271       0.00      0.00      0.00        30\n",
            "         272       0.00      0.00      0.00         8\n",
            "         273       0.00      0.00      0.00        21\n",
            "         274       0.00      0.00      0.00       123\n",
            "         275       0.03      0.01      0.02        67\n",
            "         276       0.65      0.75      0.70        20\n",
            "         277       0.00      0.00      0.00        14\n",
            "         278       0.00      0.00      0.00        19\n",
            "         279       0.86      0.50      0.63        12\n",
            "         280       0.00      0.00      0.00        15\n",
            "         281       0.74      0.82      0.78        17\n",
            "         282       0.84      0.66      0.74        41\n",
            "         283       0.00      0.00      0.00        15\n",
            "         284       0.43      0.30      0.35        74\n",
            "         285       0.31      0.32      0.31        38\n",
            "         286       0.00      0.00      0.00        16\n",
            "         287       0.00      0.00      0.00        30\n",
            "         288       0.86      0.64      0.73        28\n",
            "         289       0.00      0.00      0.00        21\n",
            "         290       0.83      0.49      0.62        41\n",
            "         291       0.09      0.08      0.09        12\n",
            "         292       0.00      0.00      0.00        24\n",
            "         293       0.29      0.20      0.24        20\n",
            "         294       0.00      0.00      0.00        23\n",
            "         295       0.00      0.00      0.00        29\n",
            "         296       0.00      0.00      0.00        28\n",
            "         297       0.00      0.00      0.00        42\n",
            "         298       0.00      0.00      0.00        53\n",
            "         299       0.00      0.00      0.00        36\n",
            "         300       1.00      0.02      0.05        41\n",
            "         301       0.12      0.22      0.16        37\n",
            "         302       1.00      0.42      0.59        26\n",
            "         303       0.00      0.00      0.00        11\n",
            "         304       0.00      0.00      0.00        31\n",
            "         305       0.44      0.24      0.31        17\n",
            "         306       0.00      0.00      0.00         9\n",
            "         307       0.00      0.00      0.00         6\n",
            "         308       0.00      0.00      0.00        34\n",
            "         309       0.53      0.23      0.32        43\n",
            "         310       0.00      0.00      0.00        30\n",
            "         311       0.11      0.16      0.13        50\n",
            "         312       0.00      0.00      0.00        24\n",
            "         313       0.00      0.00      0.00        42\n",
            "         314       0.33      0.14      0.19        22\n",
            "         315       0.00      0.00      0.00        58\n",
            "         316       0.75      0.30      0.43        10\n",
            "         317       0.00      0.00      0.00        57\n",
            "         318       0.00      0.00      0.00        10\n",
            "         319       0.00      0.00      0.00        11\n",
            "         320       0.33      0.18      0.24        11\n",
            "         321       0.33      0.38      0.35         8\n",
            "         322       0.55      0.27      0.36        22\n",
            "         323       0.91      0.36      0.51        28\n",
            "         324       0.45      0.52      0.48        50\n",
            "         325       0.00      0.00      0.00        18\n",
            "         326       0.00      0.00      0.00        33\n",
            "         327       0.00      0.00      0.00        17\n",
            "         328       0.01      0.03      0.02        29\n",
            "         329       0.14      0.14      0.14         7\n",
            "         330       0.46      0.60      0.52        10\n",
            "         331       0.00      0.00      0.00        25\n",
            "         332       0.67      1.00      0.80         2\n",
            "         333       0.19      0.36      0.25        11\n",
            "         334       0.00      0.00      0.00        24\n",
            "         335       1.00      0.20      0.33         5\n",
            "         336       0.00      0.00      0.00        33\n",
            "         337       0.40      0.07      0.11        30\n",
            "         338       0.90      0.45      0.60        42\n",
            "         339       0.00      0.00      0.00        26\n",
            "         340       0.23      0.39      0.29        36\n",
            "         341       0.10      0.23      0.14        13\n",
            "         342       1.00      0.18      0.31        11\n",
            "         343       0.00      0.00      0.00        10\n",
            "         344       0.10      0.19      0.13        21\n",
            "         345       0.00      0.00      0.00         0\n",
            "         346       0.00      0.00      0.00         6\n",
            "         347       0.00      0.00      0.00        12\n",
            "         348       0.00      0.00      0.00        13\n",
            "         349       0.50      0.08      0.14        24\n",
            "         350       0.38      0.59      0.46        27\n",
            "         351       0.22      0.12      0.15        43\n",
            "         352       0.00      0.00      0.00        30\n",
            "         353       0.21      0.23      0.22        22\n",
            "         354       0.00      0.00      0.00        31\n",
            "         355       0.54      0.70      0.61        10\n",
            "         356       0.00      0.00      0.00        20\n",
            "         357       0.30      0.60      0.40        20\n",
            "         358       0.24      0.18      0.20        28\n",
            "         359       0.29      0.38      0.33        21\n",
            "         360       0.00      0.00      0.00        25\n",
            "         361       0.45      0.40      0.42        35\n",
            "         362       0.75      0.67      0.71        36\n",
            "         363       0.22      0.24      0.23        17\n",
            "         364       0.00      0.00      0.00        13\n",
            "         365       0.00      0.00      0.00        21\n",
            "         366       0.00      0.00      0.00        18\n",
            "         367       0.00      0.00      0.00        97\n",
            "         368       0.24      0.41      0.30        29\n",
            "         369       0.80      0.33      0.47        12\n",
            "         370       0.60      0.23      0.33        13\n",
            "         371       0.06      0.06      0.06        18\n",
            "         372       0.00      0.00      0.00         6\n",
            "         373       0.00      0.00      0.00         6\n",
            "         374       0.00      0.00      0.00        30\n",
            "         375       0.00      0.00      0.00        27\n",
            "         376       0.00      0.00      0.00        28\n",
            "         377       0.00      0.00      0.00         2\n",
            "         378       0.33      0.25      0.29         4\n",
            "         379       0.00      0.00      0.00        19\n",
            "         380       0.11      0.40      0.17         5\n",
            "         381       0.75      0.17      0.27        18\n",
            "         382       0.00      0.00      0.00        22\n",
            "         383       0.00      0.00      0.00        16\n",
            "         384       0.33      0.15      0.21        13\n",
            "         385       0.00      0.00      0.00        18\n",
            "         386       0.50      0.27      0.35        11\n",
            "         387       0.00      0.00      0.00        88\n",
            "         388       0.00      0.00      0.00        13\n",
            "         389       0.00      0.00      0.00         6\n",
            "         390       0.00      0.00      0.00         6\n",
            "         391       0.96      0.51      0.67        51\n",
            "         392       0.00      0.00      0.00        13\n",
            "         393       0.30      0.19      0.23        37\n",
            "         394       0.00      0.00      0.00         6\n",
            "         395       0.00      0.00      0.00         9\n",
            "         396       0.00      0.00      0.00        13\n",
            "         397       1.00      0.17      0.29         6\n",
            "         398       0.40      0.21      0.27        29\n",
            "         399       0.89      0.76      0.82        33\n",
            "         400       0.25      0.03      0.06        31\n",
            "         401       0.20      0.08      0.11        50\n",
            "         402       0.92      0.67      0.77        18\n",
            "         403       0.02      0.14      0.03         7\n",
            "         404       0.52      0.50      0.51        26\n",
            "         405       0.86      0.11      0.19        56\n",
            "         406       1.00      1.00      1.00         4\n",
            "         407       0.31      0.24      0.27        17\n",
            "         408       0.00      0.00      0.00        11\n",
            "         409       0.00      0.00      0.00        18\n",
            "         410       0.50      0.30      0.37        10\n",
            "         411       0.00      0.00      0.00        45\n",
            "         412       0.26      0.45      0.33        20\n",
            "         413       0.50      0.24      0.32        25\n",
            "         414       0.00      0.00      0.00        20\n",
            "         415       0.40      0.33      0.36         6\n",
            "         416       0.00      0.00      0.00        26\n",
            "         417       0.00      0.00      0.00        10\n",
            "         418       0.00      0.00      0.00        18\n",
            "         419       0.20      0.33      0.25         6\n",
            "         420       0.35      0.35      0.35        17\n",
            "         421       0.00      0.00      0.00         1\n",
            "         422       0.00      0.00      0.00         6\n",
            "         423       0.00      0.00      0.00        12\n",
            "         424       0.00      0.00      0.00         4\n",
            "         425       0.00      0.00      0.00        11\n",
            "         426       0.00      0.00      0.00        11\n",
            "         427       0.55      0.75      0.63         8\n",
            "         428       0.00      0.00      0.00        26\n",
            "         429       0.31      0.10      0.15        40\n",
            "         430       0.00      0.00      0.00         2\n",
            "         431       0.00      0.00      0.00        35\n",
            "         432       0.00      0.00      0.00        15\n",
            "         433       0.00      0.00      0.00        18\n",
            "         434       0.00      0.00      0.00         0\n",
            "         435       0.00      0.00      0.00         0\n",
            "         436       0.00      0.00      0.00        28\n",
            "         437       0.33      0.12      0.18        33\n",
            "         438       0.40      0.40      0.40        20\n",
            "         439       0.00      0.00      0.00        36\n",
            "         440       0.00      0.00      0.00        18\n",
            "         441       0.40      0.33      0.36        18\n",
            "         442       0.56      0.31      0.40        16\n",
            "         443       0.14      0.23      0.18        22\n",
            "         444       0.11      0.17      0.13         6\n",
            "         445       0.63      0.57      0.60        21\n",
            "         446       0.84      0.35      0.49        46\n",
            "         447       0.00      0.00      0.00        69\n",
            "         448       0.00      0.00      0.00         7\n",
            "         449       0.09      0.33      0.14         3\n",
            "         450       0.00      0.00      0.00        52\n",
            "         451       0.00      0.00      0.00        16\n",
            "         452       0.73      0.47      0.57        17\n",
            "         453       0.00      0.00      0.00        13\n",
            "         454       0.31      0.45      0.37        11\n",
            "         455       0.00      0.00      0.00        12\n",
            "         456       0.17      0.17      0.17         6\n",
            "         457       0.00      0.00      0.00        18\n",
            "         458       0.00      0.00      0.00        15\n",
            "         459       0.62      0.18      0.28        28\n",
            "         460       0.00      0.00      0.00        18\n",
            "         461       0.17      0.40      0.24        10\n",
            "         462       0.50      0.17      0.25        24\n",
            "         463       0.00      0.00      0.00        18\n",
            "         464       0.00      0.00      0.00        39\n",
            "         465       0.00      0.00      0.00        11\n",
            "         466       0.15      0.14      0.15        35\n",
            "         467       0.20      0.38      0.26        21\n",
            "         468       0.24      0.11      0.15        37\n",
            "         469       0.00      0.00      0.00         5\n",
            "         470       0.00      0.00      0.00         8\n",
            "         471       0.44      0.22      0.29        37\n",
            "         472       0.00      0.00      0.00        47\n",
            "         473       0.50      0.21      0.30        14\n",
            "         474       0.79      0.48      0.59        23\n",
            "         475       0.00      0.00      0.00        66\n",
            "         476       0.00      0.00      0.00         3\n",
            "         477       0.36      0.21      0.27        19\n",
            "         478       0.00      0.00      0.00         1\n",
            "         479       0.00      0.00      0.00        23\n",
            "         480       0.00      0.00      0.00        60\n",
            "         481       0.00      0.00      0.00        26\n",
            "         482       0.00      0.00      0.00         4\n",
            "         483       0.00      0.00      0.00         8\n",
            "         484       0.82      0.39      0.53        23\n",
            "         485       0.60      0.17      0.26        18\n",
            "         486       0.36      0.42      0.38        12\n",
            "         487       0.00      0.00      0.00        29\n",
            "         488       1.00      1.00      1.00         1\n",
            "         489       0.00      0.00      0.00         6\n",
            "         490       0.00      0.00      0.00         7\n",
            "         491       0.00      0.00      0.00         3\n",
            "         492       0.00      0.00      0.00        10\n",
            "         493       0.27      0.68      0.38        19\n",
            "         494       0.00      0.00      0.00         7\n",
            "         495       0.38      0.75      0.50         8\n",
            "         496       0.11      0.11      0.11        18\n",
            "         497       0.00      0.00      0.00        72\n",
            "         498       0.25      0.12      0.17         8\n",
            "         499       0.00      0.00      0.00        32\n",
            "\n",
            "   micro avg       0.53      0.27      0.36     37472\n",
            "   macro avg       0.28      0.21      0.22     37472\n",
            "weighted avg       0.45      0.27      0.32     37472\n",
            " samples avg       0.35      0.26      0.28     37472\n",
            "\n",
            "Time taken to run this cell : 0:10:49.011459\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0lxakvZYHFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "    \n",
        "x = PrettyTable()\n",
        "\n",
        "x.field_names = [\"Model\", \"Micro f-score\", \"Hamming loss\", \"Precision\" , \"Recall\"]\n",
        "\n",
        "x.add_row([\"Linear Regression\",0.3658, 0.0036648 ,0.5203 ,0.2821])\n",
        "x.add_row([\"Linear SVM \",0.3560, 0.0036292 , 0.5313 , 0.2676])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SxWe8nAZQEe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f362b03b-feda-41c7-b672-91769db17ea1"
      },
      "source": [
        "print(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+---------------+--------------+-----------+--------+\n",
            "|       Model       | Micro f-score | Hamming loss | Precision | Recall |\n",
            "+-------------------+---------------+--------------+-----------+--------+\n",
            "| Linear Regression |     0.3658    |  0.0036648   |   0.5203  | 0.2821 |\n",
            "|    Linear SVM     |     0.356     |  0.0036292   |   0.5313  | 0.2676 |\n",
            "+-------------------+---------------+--------------+-----------+--------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIK2qyKAUHRq",
        "colab_type": "text"
      },
      "source": [
        "## Conclusions:\n",
        "\n",
        "- A multilabel classification problem is solved with some simple machine learning models like logistic and linear SVM by One VS Rest classifier.\n",
        "- As the data is very huge with so many tags, therefore due to memory constraints we took only 100k data points and 500 tags .\n",
        "- Feature engineering is done on the body of the question by preprocessing it with BOW 2 grams.\n",
        "- **More weightage is given to the title(3times) of the question as it is more powerful in predicting the tag.**\n",
        "- Since some tags occur much more frequenctly than others, Micro-averaged F1-score is the appropriate metric for this probelm. Along with that hamming loss is also considered.\n",
        "- Multilearn libraries , Decision trees or Random forests can also be implemented but it takes much more time to compute. So we limited ourselves to LR AND Linear SVM.\n",
        "- **The results are plotted in a table above and the f1-score is not very high as we took only a small part of the data 100k out of 6M data points. And also Accuracy is very low as it is not the correct measure for this problem.**  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pNj7NJSYGDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}